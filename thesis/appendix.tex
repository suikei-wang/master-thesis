\chapter{Appendix}
\renewcommand{\thesection}{\Alph{section}}
\label{appendix}
\section{An Overview of Numerical Optimization}
\subsection{Theory of Optimization}

\subsubsection{Proof of Theorem~\ref{thm:23}}
\label{appendix:trm23}
\begin{proof}
Let 
$$
m:=\inf \{f(x): x \in \Omega\}
$$
\par By the definition of $m$ we may pick a sequence $\left\{x_{k}\right\} \subset \Omega$ with $f\left(x_{k}\right) \rightarrow m$ as $k \rightarrow \infty$. Because $\Omega$ is compact, we can extract a convergent subsequence $\left\{x_{k_j}\right\}$ from $\left\{x_{k}\right\}$. Let $x^{*} \in \Omega$ denote the limit point of $\left\{x_{k_j}\right\}$. Since $f$ is continuous, $f\left(x^{*}\right)=\lim _{j \rightarrow \infty} f\left(x_{k_{j}}\right)=m$. Thus $m$ is finite and $x^{*}$ is a global minimizer of $f$ on $\Omega$. 
\par When $\Omega = \mathbb{R}^n$, we need to impose conditions on f at infinity to guarantee the existence of a global minimizer.
\end{proof}

\subsubsection{Proof of Theorem~\ref{thm:24}}
\label{appendix:trm24}
\begin{proof}
Let $m:=\inf \left\{f(x): x \in \mathbb{R}^{n}\right\}$, and take a sequence $\left\{x_{k}\right\}$ such that 
$$
f\left(x_{k}\right) \rightarrow m \quad \textrm{as }  k \rightarrow \infty .
$$
\par Since $f$ is coervice, $\left\{x_{k}\right\}$ must be bounded; otherwise it has a subsequence $\left\{x_{k_j}\right\}$ with $\left\|x_{k_{j}}\right\| \rightarrow \infty$ as $j \rightarrow \infty$, and hence $m=\lim _{j \rightarrow \infty} f\left(x_{k_{j}}\right)=+\infty$, a contradition. 
\par Thus there is $r > 0$ such that
$$
\left\{x_{k}\right\} \subset\left\{x \in \mathbb{R}^{n}:\left\|x_{k}\right\| \leq r\right\}.
$$
\par Because $\left\{x \in \mathbb{R}^{n}:\|x\| \leq r\right\}$ is compact, $\left\{x_{k}\right\}$ has a convergent subsequence $\left\{x_{k_j}\right\}$ with $x_{k_{j}} \rightarrow x^{*}$ as $j \rightarrow \infty$. In view of the continuity of $f$, we have 
$$
f\left(x^{*}\right)=\lim _{j \rightarrow \infty} f\left(x_{k_{j}}\right)=m
$$
\par Therefore $m$ is finite and $f$ achieves its minimum on $\mathbb{R}^n$ at $x^{*}$
\end{proof}
\subsubsection{Proof of Theorem~\ref{thm:25}}
\label{appendix:thm25}
\begin{proof}
We may assume that $\alpha>f_{*}:=\inf \left\{f(x): x \in \mathbb{R}^{n}\right\}$. Let $\left\{x_{k}\right\}$ be a minimizing sequence for $f$, i.e.
$$
f\left(x_{k}\right) \rightarrow f_{*} \quad \textrm { as }  k \rightarrow \infty
$$
\par Then there is an $N$ such that $f(x_k) \leq \alpha$ for all $k \geq N$, that is, $x_k \in D$ for all $k \geq N$. Since $D$ is compact, $\left\{x_{k}\right\}_{k=N}^{\infty}$ has a convergent subsequence $\left\{x_{k_j}\right\}$ with $x_{k_{j}} \rightarrow x_{*} \in D$ as $j \rightarrow \infty$. In view of the lower semi-continuity of $f$, we have
$$
f\left(x_{*}\right) \leq \lim _{j \rightarrow \infty} f\left(x_{k_{j}}\right)=f_{*}
$$
\par By the definition of $f_*$ we must have $f(x_{*}) = f_*$. Therefore $f$ achieves its minimum on $\mathbb{R}$ at $x_{*}$.  
\end{proof}
\subsubsection{Proof of Theorem~\ref{thm28}}
\label{appendix:thm28}
\begin{proof}
(NC1): First, recall that for any $v \in \mathbb{R}^n$ there holds
$$
v^{T} \nabla f\left(x^{*}\right)=D_{v} f\left(x^{*}\right)=\lim _{t \searrow 0} \frac{f\left(x^{*}+t v\right)-f\left(x^{*}\right)}{t}.
$$
\par Since $x^*$ is a local minimizer, we have
$$
f\left(x^{*}+t v\right)-f\left(x^{*}\right) \geq 0 \quad \textrm { for small }|t|.
$$
\par Therefore
$$
v^{T} \nabla f\left(x^{*}\right) \geq 0 \quad \textrm { for all } v \in \mathbb{R}^{n}.
$$
\par In particular this implies $(-v)^{T} \nabla f\left(x^{*}\right) \geq 0$ and thus 
$$
v^{T} \nabla f\left(x^{*}\right) \leq 0 \quad \textrm { for all } v \in \mathbb{R}^{n}.
$$
\par Therefore $v^{T} \nabla f(x^{*})=0$ for all $v \in \mathbb{R}^{n}$. Taking $v=\nabla f(x^*)$ gives $\|\nabla f(x^*)\|^2 = 0$ which shows that $\nabla f(x^{*})=0$ 
\end{proof}
\begin{proof}
(NC2): Recall that for any $v \in \mathbb{R}^n$ and small $t > 0$ there is $0 < s < 1$ such that 
$$
f\left(x^{*}+t v\right)=f\left(x^{*}\right)+t v^{T} \nabla f\left(x^{*}\right)+\frac{1}{2} t^{2} v^{T} \nabla^{2} f\left(x^{*}+s t v\right) v.
$$
\par Since $x^*$ is a local minimizer of $f$, we have $f\left(x^{*}+t v\right) \geq f\left(x^{*}\right)$ and $\nabla f(x^{*})=0$ by (NC1). Therefore
$$
\frac{1}{2} t^{2} v^{T} \nabla^{2} f\left(x^{*}+s t v\right) v=f\left(x^{*}+t v\right)-f\left(x^{*}\right) \geq 0 .
$$
\par This implies that
$$
v^{T} \nabla^{2} f\left(x^{*}+s t v\right) v \geq 0.
$$
\par Taking $t \rightarrow 0$ gives
$$
v^{T} \nabla^{2} f\left(x^{*}\right) v \geq 0 \quad \textrm { for all } v \in \mathbb{R}^{n}
$$ 
i.e. $\nabla^2 f(x^*)$ is semi-definite.
\end{proof}
\begin{proof}
(SC1): Since $\nabla^2 f(x$ is continuous and $\nabla^2 f(x^*) \geq 0$, we can find $r > 0$ such that
$$
B_{r}\left(x^{*}\right) \subset \Omega \quad \textrm { and } \quad \nabla^{2} f(x)>0 \textrm { for all } x \in B_{r}\left(x^{*}\right).
$$
\par By Taylor’s formula we have
$$
f(x)=f\left(x^{*}\right)+\nabla f\left(x^{*}\right) \cdot\left(x-x^{*}\right)+\frac{1}{2}\left(x-x^{*}\right)^{T} \nabla^{2} f(\hat{x})\left(x-x^{*}\right)
$$
where $\hat{x} := x^* + t(x - x^*)$ for some $0 < t < 1$.
\par It is clear that $\hat{x} \in B_{r}\left(x^{*}\right)$ and hence $\nabla^2f(\hat{x}) > 0$ which implies that 
$$
\left(x-x^{*}\right)^{T} \nabla^{2} f(\hat{x})\left(x-x^{*}\right)>0 \quad \textrm { for } x \neq x^{*}
$$
\par Consequently
$$
f(x)>f\left(x^{*}\right)+\nabla f\left(x^{*}\right) \cdot\left(x-x^{*}\right)
$$
for all $x \in B_{r}\left(x^{*}\right)$ with $x \neq x^*$. 
\par Since $\nabla f(x^*) = 0$, we can obtain $f(x) > f(x^*)$ for all $x \in B_{r}\left(x^{*}\right)$ with $x \neq x^*$. 
\end{proof}

\subsubsection{Proof of Lemma~\ref{lemma:210}}
\label{appendix:lemma210}
\begin{proof}
    For $d \in T_{x^{*}} \mathscr{F}$, we have ${z_k} \subset \mathscr{F}$ and ${t_k}$ such that
    $$
    z_{k} \rightarrow x^{*}, \quad 0<t_{k} \rightarrow 0 \quad \textrm { and } \quad \frac{z_{k}-x^{*}}{t_{k}} \rightarrow d
    $$
    as $k \rightarrow \infty$. As $f(x^*) \leq f(z_k)$, by Taylor’s formula we have
    $$
    \begin{aligned} f\left(x^{*}\right) & \leq f\left(z_{k}\right)=f\left(x^{*}+\left(z_{k}-x^{*}\right)\right) \\ &=f\left(x^{*}\right)+\left(z_{k}-x^{*}\right)^{T} \nabla f\left(x^{*}\right)+\frac{1}{2}\left(z_{k}-x^{*}\right)^{T} \nabla^{2} f\left(\hat{z}_{k}\right)\left(z_{k}-x^{*}\right) \end{aligned}
    $$
    where $\hat{z}_{k}$ is a point on the line segment joining $x^*$ and $z_k$. This implies that
    $$
    0 \leq\left(\frac{z_{k}-x^{*}}{t_{k}}\right)^{T} \nabla f\left(x^{*}\right)+\frac{1}{2}\left(z_{k}-x^{*}\right)^{T} \nabla^{2} f\left(\hat{z}_{k}\right)\left(\frac{z_{k}-x^{*}}{t_{k}}\right)
    $$
    \par Letting $k \rightarrow \infty$ gives $d^{T} \nabla f\left(x^{*}\right) \geq 0$
\end{proof}

\subsection{Solution of Unconstrained and Constrained Optimization Problems}
\subsubsection{Proof of Equation~\ref{equ:2.3}}
\label{appendix:equ2.3}
\begin{proof}
    Firstly, for any optimal $y$, according to the first-order optimality condition, we have
    $$
    \frac{df(x, y)}{dy} = \boldsymbol{0} \in \mathbb{R}^{1 \times m}
    $$
    \par Then from the implicit function theorem, rearranging and differentiating both sides we have
    $$
    \begin{aligned}
        \operatorname{D}(\frac{df(x, y)}{dy})^T &= \boldsymbol{0} \in \mathbb{R}^{m \times n} \\
        &= \frac{\partial^{2}}{\partial x \partial y}f(x,y) + \frac{\partial^2}{\partial y^2}f(x, y) \frac{dy(x)}{dx}
    \end{aligned}
    $$
    $$ \frac{dy(x)}{dx} = -[(\frac{\partial^2}{\partial y^2})f(x,y)]^{-1} (\frac{\partial^2}{\partial x \partial y})f(x, y)$$
\end{proof}

\subsubsection{Proof of Equation~\ref{equ:2.4}}
\label{appendix:equ2.4}
\begin{proof}
    According to the definition of Lagrange multipliers, we can define the Lagrangian:
    $$
    \mathcal{L}(x, y, \lambda)=f(x, y)-\sum_{i=1}^{p} \lambda_{i} (A_{i}y_i - b_i)
    $$
    \par We are going to find the stationary point $(y, \lambda)$ for this lagrangian. Therefore, we calculate the derivative of $\mathcal{L}$ with respect to $y$ and $\lambda$ separately: 
    \begin{equation}
        \label{equ:apd:241}
        \frac{\partial}{\partial y} f(x,y) - \sum_{i=1}^{p} \lambda_{i} \frac{\partial}{\partial y}(A_{i}y_i - b_i) = 0
    \end{equation}
    
    \begin{equation}
        A \boldsymbol{y} - \boldsymbol{b} = 0
    \end{equation}
    \par Since $y$ is the optimal point, we have $\frac{\partial}{\partial y} f(x,y) = 0$, which can be an unconstrained problem or it is orthogonal to the constraint surface. For unconstrained cases, we can set $\lambda = 0$ directly. For the orthogonal case, from Equation ~\ref{equ:apd:241}, we have
    $$
    \frac{\partial}{\partial y} f(x,y) = \sum_{i=1}^{p} \lambda_{i} \frac{\partial}{\partial y}(A_{i}y_i - b_i) = \lambda^T A
    $$
    
\end{proof}