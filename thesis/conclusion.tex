\chapter{Conclusion}
\label{cha:conc}
In this thesis, we gave readers a thorough overview of constrained optimization in the deep declarative network: the multiple constraints nodes (PART I) and solutions for non-regular points (PART II). 
\par In Chapter~\ref{cha:overviewpart1}, we walked through the theory of numerical optimization, which is the theoretical background of the deep declarative network. A system explanation of the optimality conditions of constrained optimization problems under the assumption of convexity is given: the objective function is differentiable at the optimal point and twice differentiable on the definition set. Solutions to unconstrained and constrained optimization problems are discussed, which are obtained based on the optimality conditions with Lagrange multipliers. We also introduced the related works in the differentiable network, which is different from the traditional deep neural network using the properties of back-propagation with differentiable optimization problems to construct the nodes.
\par In Chapter~\ref{cha:ddn}, we covered all contents of the deep declarative network. More specifically, the properties and functions of deep declarative nodes. We introduced how the deep declarative network works and its learning progress based on the end-to-end learnable declarative nodes. We later presented the back-propagation and corresponding gradient solution of regular points through declarative nodes in different scenarios: unconstrained, equality constrained and inequality constrained. We also showed various examples of our implementation of deep declarative nodes in different scenarios. We hope our works in this new type of differentiable network will eventually lead to a new generation of deep neural networks.
\par In PART II, the key questions we wanted to answer are: Is the solution of the deep declarative nodes always regular? If the solution is non-regular, how do we find the gradient then perform the back-propagation? 
\par In Chapter~\ref{cha:overviewpart2}, we addressed the problems in regular deep declarative nodes, which is based on the assumption of convexity in the optimality conditions. We separated the problems into three scenarios: the overdetermined system, rank deficiency problems and non-convex cases. We also provided the corresponding example for each scenario, which is not able to obtain the gradient directly. Lastly, we discussed multiple previous works in solving non-regular solution problems. 
\par In Chapter~\ref{cha:result}, we showed that we can deal with the different non-regular solutions with both approximation and theoretical based optimality methods. For overdetermined systems and rank deficiency problems, we introduced approaches based on the least-squares method and iterative gradient descent. Both of these methods try to approximate the closest solution with minimum residual errors. For non-convex cases, we proposed a different algorithm for solving the constrained optimization problems based on the optimality conditions regardless of the convexity. This technique can solve the problem exactly with the weighted Chebyshev norm and non-linear Lagrangian. We believe these methods are useful for handling all range of solutions in the deep declarative network. 
\par All together, we are really excited about the progress that has been made in this field for the past years and have been glad to be able to contribute to this field. At the same time, we also hope to encourage e more researchers to work on the applications, or apply the deep declarative network to new domains or tasks. We believe that it will lead us towards building better constrained problem solvers and hope to see these ideas implemented and developed in industry applications.